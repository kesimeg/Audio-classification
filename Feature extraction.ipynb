{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae354eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4a076",
   "metadata": {},
   "source": [
    "As we have said in data analysis part we will use only a portion of the dataset \n",
    "(which were violin ðŸŽ», saxophone ðŸŽ·, hihat ðŸŽ¼, acoustic_guitar ðŸŽ¸ and laughter ðŸ¤£ classes)\n",
    "We will encode each class name with 2 to 3 letters for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3cbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = ['Acoustic_guitar','Hi-hat','Laughter','Saxophone','Violin_or_fiddle']   \n",
    "\n",
    "def label_to_name(label):\n",
    "    if label == 'Acoustic_guitar':\n",
    "        return \"gui\"\n",
    "    elif label == 'Hi-hat':\n",
    "        return \"hi\"\n",
    "    elif label == 'Laughter':\n",
    "        return \"lau\"\n",
    "    elif label == 'Saxophone':\n",
    "        return \"sax\"\n",
    "    elif label == 'Violin_or_fiddle':\n",
    "        return \"vio\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230ca4e",
   "metadata": {},
   "source": [
    "Now we will load our metadata to find the samples that belong to classes we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35a232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_dir = \"./data/FSDKaggle2018.meta\"\n",
    "file = pd.read_csv(os.path.join(train_audio_dir,\"train_post_competition.csv\"),usecols=['fname','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94212c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the data into a dictionary of file names as keys and labels as values ex: example_samp.wav -> Saxophone\n",
    "audio_dict = file.set_index('fname')['label'].to_dict() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d04f53",
   "metadata": {},
   "source": [
    "Now we will extract features from our audio data. \n",
    "This will help us get meaningful inputs and reduce the dimensionality. \n",
    "We will use melspectogram as our feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f651b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9473/9473 [05:19<00:00, 29.60it/s] \n"
     ]
    }
   ],
   "source": [
    "train_audio_dir = \"./data/FSDKaggle2018.audio_train\"\n",
    "file_list = os.listdir(train_audio_dir)\n",
    "file_list.sort()\n",
    "\n",
    "#We create a directory to save each of our feature files\n",
    "save_dir = \"./data/melspectrogram\"\n",
    "train_save_dir = os.path.join(save_dir,\"train\")\n",
    "if not os.path.exists(train_save_dir):\n",
    "    os.makedirs(train_save_dir)\n",
    "    \n",
    "\n",
    "for file in tqdm(file_list):\n",
    "    label = audio_dict[file]\n",
    "    if label in selected_classes:\n",
    "        data , sr = librosa.load(os.path.join(train_audio_dir,file),sr=16000) # load the audio sample\n",
    "        hop = sr//25 #hop length is selected as sampling_rate//25 (25Hz of data), you can try other values\n",
    "        \n",
    "        short_label = label_to_name(label) # encode the label\n",
    "        \n",
    "        data = data[:sr*6] # if the sample is longer than 6 seconds we crop it\n",
    "        if data.shape[0] != sr*6:\n",
    "            data = np.resize(data,6*sr)\n",
    "        \n",
    "        melspec = librosa.feature.melspectrogram(data, n_mels = 60,hop_length=hop,n_fft=hop*2)\n",
    "        features = librosa.core.amplitude_to_db(melspec)\n",
    "        torch.save(features,os.path.join(train_save_dir,file[:-4]+\"_\"+short_label+\".pt\"))# save data as torch tensor\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79816336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:56<00:00, 28.28it/s]\n"
     ]
    }
   ],
   "source": [
    "val_audio_dir = \"./data/FSDKaggle2018.meta\"\n",
    "file = pd.read_csv(os.path.join(val_audio_dir,\"test_post_competition_scoring_clips.csv\"),usecols=['fname','label'])\n",
    "audio_dict = file.set_index('fname')['label'].to_dict()\n",
    "\n",
    "val_audio_dir = \"./data/FSDKaggle2018.audio_test\"\n",
    "file_list = os.listdir(val_audio_dir)\n",
    "file_list.sort()\n",
    "\n",
    "save_dir = \"./data/melspectrogram\"\n",
    "val_save_dir = os.path.join(save_dir,\"validation\")\n",
    "if not os.path.exists(val_save_dir):\n",
    "    os.makedirs(val_save_dir)\n",
    "\n",
    "\n",
    "for file in tqdm(file_list):\n",
    "    label = audio_dict[file]\n",
    "    if label in selected_classes:\n",
    "        data , sr = librosa.load(os.path.join(val_audio_dir,file),sr=16000)\n",
    "        hop = sr//25\n",
    "        short_label = label_to_name(label)\n",
    "        data = data[:sr*6]\n",
    "        if data.shape[0] != sr*6:\n",
    "            data = np.resize(data,6*sr)\n",
    "        melspec = librosa.feature.melspectrogram(data, n_mels = 60,hop_length=hop,n_fft=hop*2)\n",
    "        features = librosa.core.amplitude_to_db(melspec)\n",
    "        torch.save(features,os.path.join(val_save_dir,file[:-4]+\"_\"+short_label+\".pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76454435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
